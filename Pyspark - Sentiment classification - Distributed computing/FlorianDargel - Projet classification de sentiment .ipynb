{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acef540b",
   "metadata": {},
   "source": [
    "# Verification des versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d17b7b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:42.091583Z",
     "start_time": "2022-03-18T18:47:41.982372Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.5 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34af31f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:43.763200Z",
     "start_time": "2022-03-18T18:47:42.093578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, Java HotSpot(TM) Client VM, 1.8.0_251\n",
      "Branch \n",
      "Compiled by user  on 2019-08-27T21:21:38Z\n",
      "Revision \n",
      "Url \n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfa86c",
   "metadata": {},
   "source": [
    "# Lancement de la session spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720823bf",
   "metadata": {},
   "source": [
    "SparkSession dans Spark 2.0 fournit une prise en charge intégrée des fonctionnalités Hive, notamment la possibilité d'écrire des requêtes à l'aide de HiveQL, l'accès aux UDF Hive et la possibilité de lire les données des tables Hive. Pour utiliser ces fonctionnalités, vous n'avez pas besoin d'avoir une configuration Hive existante. \n",
    "\n",
    "J'utilise spark SQL. Ceci nous permet d'introduire un objet : le DataFrame en spark. Il s'agit d'un objet proche du RDD, mais qui permet de stocker de manière distribuée des données strucutrées, là ou les RDD nous permettent de stocker des données non strucutrées. Il se rapproche très fortement du DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a85a496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:43.779158Z",
     "start_time": "2022-03-18T18:47:43.764198Z"
    }
   },
   "outputs": [],
   "source": [
    "#On importe findspark\n",
    "import findspark\n",
    "#On initialise findspark pour identifier nos chemins Spark\n",
    "findspark.init()\n",
    "#On importe spark session\n",
    "from pyspark.sql import SparkSession\n",
    "#On crée une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName (\"Projet calcul distribué\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf9235",
   "metadata": {},
   "source": [
    "# Importation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ffe32",
   "metadata": {},
   "source": [
    "J'importe mon fichier d'entrainement $\\textbf{train.json}$ qui est en format json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45059dfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:47.341209Z",
     "start_time": "2022-03-18T18:47:43.785142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.json(\"C:/Users/na_to/OneDrive/Bureau/Insa/Mapromo/Calcul_distribue/Projet/train.json\")\n",
    "#4 partitions sur l'importation de données : voir http://localhost:4040 : données distribué RDD \n",
    "\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8c2c6",
   "metadata": {},
   "source": [
    "On a deux variables présentes dans notre base de données : message et polarity. Polarity et message sont des variables \"string\" . On doit les convertir en format numérique pour appliquer un modèle de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f03d3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:48.756303Z",
     "start_time": "2022-03-18T18:47:47.343204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|       0|61475|\n",
      "|       4|66926|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby(\"polarity\")\\\n",
    "     .count()\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cb42b",
   "metadata": {},
   "source": [
    "On a deux valeurs à prédire 4 et 0. On remarque que les données sont équilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a645347a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:48.945929Z",
     "start_time": "2022-03-18T18:47:48.758300Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import * \n",
    "train_data = train_data.select(\"message\", col(\"polarity\").cast(\"Int\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91508a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T18:43:00.011201Z",
     "start_time": "2022-03-16T18:43:00.005218Z"
    }
   },
   "source": [
    "On a convertit notre variable cible en entier et nommé la variable cible \"Polarity\" par \"label\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee59a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:57.720722Z",
     "start_time": "2022-03-18T18:47:57.556530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             message|label|\n",
      "+--------------------+-----+\n",
      "|! Comment était l...|    0|\n",
      "|! d'accord! Va-t-...|    0|\n",
      "|!!! Taihen desu n...|    0|\n",
      "|!!!! Auto-dj .. c...|    0|\n",
      "|!!!! Ce n'est que...|    0|\n",
      "|\"Aimant\" Le jour ...|    0|\n",
      "|\"Attrape-moi si t...|    0|\n",
      "|\"Beverley Knight\"...|    0|\n",
      "|\"Crack ... break ...|    0|\n",
      "|\"Désolé\" une cond...|    0|\n",
      "|\"Effrayant\" Pense...|    0|\n",
      "|\"En espérant qu'i...|    0|\n",
      "|\"Feelin on my a\" ...|    0|\n",
      "|\"Graisse réduite\"...|    0|\n",
      "|\"Je ne sais pas s...|    0|\n",
      "|\"Je regarde la té...|    0|\n",
      "|\"Je suis fatigué ...|    0|\n",
      "|\"Les deux adultes...|    0|\n",
      "|\"Les étoiles dans...|    0|\n",
      "|\"Nous avons vraim...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e80766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:47:59.754463Z",
     "start_time": "2022-03-18T18:47:59.604820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(message='! Comment était la terre des écossais?', label=0),\n",
       " Row(message=\"! d'accord! Va-t-il apporter des armes à feu et des voitures? Nous n'avons pas ici ici des axes et des cavaliers ... o_o\", label=0),\n",
       " Row(message='!!! Taihen desu ne. Une idée de quoi ou qui a causé cela?', label=0),\n",
       " Row(message=\"!!!! Auto-dj .. ce serait un peu dope lol ... sauf que cela me met hors d'un travail\", label=0),\n",
       " Row(message=\"!!!! Ce n'est que hier que j'ai vu que les flambeaux continuaient à avoir des pains croisés chauds ne doivent pas être partout\", label=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ed6ff",
   "metadata": {},
   "source": [
    "Les tweet contienent beaucoup d’aléatoire qui nuit à l’estimation des modèles : les minuscules, les majuscules, les signes de ponctuation… On peut les garder mais plus de variabilité implique plus de données pour les apprendre. On préfère alors le nettoyer avant de le découper en mot (ou caractères ou syllabe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f046d5",
   "metadata": {},
   "source": [
    "# Suppression des ponctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6f172",
   "metadata": {},
   "source": [
    "on remarque qu'on a beaucoup de ponctuation dans les tweet, il serait intéressant de les supprimer pour réduire nos variables dans l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7205911c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:01.326203Z",
     "start_time": "2022-03-18T18:48:01.320219Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Supprime la ponctuation, passe en minuscules et supprime les espaces de début et de fin..\n",
    "\n",
    "    Note:\n",
    "        Seuls les espaces, les lettres et les chiffres doivent être conservés.  \n",
    "\n",
    "    Args:\n",
    "        colonne (Column): Une colonne contenant une phrase.\n",
    "\n",
    "    Returns:\n",
    "        Colonne : une colonne nommée \"message\" avec des opérations de nettoyage appliquées.\n",
    "    \"\"\"\n",
    "    return lower(trim(regexp_replace(column,'\\\\p{Punct}',''))).alias('message')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6d3b7",
   "metadata": {},
   "source": [
    "lower : Pour mettre les tweets en minuscules\n",
    "\n",
    "\\\\\\p{Punct} : la classe de caractères pour la ponctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0af534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:03.304240Z",
     "start_time": "2022-03-18T18:48:03.179664Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_train = train_data.select(removePunctuation(col('message')),\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32078e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:03.788106Z",
     "start_time": "2022-03-18T18:48:03.643410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             message|label|\n",
      "+--------------------+-----+\n",
      "|comment était la ...|    0|\n",
      "|daccord vatil app...|    0|\n",
      "|taihen desu ne un...|    0|\n",
      "|autodj  ce serait...|    0|\n",
      "|ce nest que hier ...|    0|\n",
      "|aimant le jour de...|    0|\n",
      "|attrapemoi si tu ...|    0|\n",
      "|beverley knight p...|    0|\n",
      "|crack  break  sha...|    0|\n",
      "|désolé une condit...|    0|\n",
      "|effrayant penser ...|    0|\n",
      "|en espérant quil ...|    0|\n",
      "|feelin on my a qu...|    0|\n",
      "|graisse réduite b...|    0|\n",
      "|je ne sais pas si...|    0|\n",
      "|je regarde la tél...|    0|\n",
      "|je suis fatigué d...|    0|\n",
      "|les deux adultes ...|    0|\n",
      "|les étoiles dans ...|    0|\n",
      "|nous avons vraime...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0afd55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:04.114306Z",
     "start_time": "2022-03-18T18:48:03.981006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(message='comment était la terre des écossais', label=0),\n",
       " Row(message='daccord vatil apporter des armes à feu et des voitures nous navons pas ici ici des axes et des cavaliers  oo', label=0),\n",
       " Row(message='taihen desu ne une idée de quoi ou qui a causé cela', label=0),\n",
       " Row(message='autodj  ce serait un peu dope lol  sauf que cela me met hors dun travail', label=0),\n",
       " Row(message='ce nest que hier que jai vu que les flambeaux continuaient à avoir des pains croisés chauds ne doivent pas être partout', label=0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_train.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737c3dc",
   "metadata": {},
   "source": [
    "Dans cette partie, on peut remarquer que les ponctuations ont bien été supprimées et que les caractères sont tous en minuscules. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827d7da",
   "metadata": {},
   "source": [
    "# Ajout de variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185216af",
   "metadata": {},
   "source": [
    "On ajoute une nouvelle variable $\\textbf{lenght_of_message}$ qui consiste à compter le nombre de caractère par tweet. Cette variable on l'ajoutera lors de la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e93831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:06.859772Z",
     "start_time": "2022-03-18T18:48:06.703283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|             message|label|length_of_message|\n",
      "+--------------------+-----+-----------------+\n",
      "|comment était la ...|    0|               35|\n",
      "|daccord vatil app...|    0|              108|\n",
      "|taihen desu ne un...|    0|               51|\n",
      "|autodj  ce serait...|    0|               72|\n",
      "|ce nest que hier ...|    0|              119|\n",
      "|aimant le jour de...|    0|               28|\n",
      "|attrapemoi si tu ...|    0|              111|\n",
      "|beverley knight p...|    0|              145|\n",
      "|crack  break  sha...|    0|               39|\n",
      "|désolé une condit...|    0|              102|\n",
      "|effrayant penser ...|    0|               50|\n",
      "|en espérant quil ...|    0|              138|\n",
      "|feelin on my a qu...|    0|               60|\n",
      "|graisse réduite b...|    0|              130|\n",
      "|je ne sais pas si...|    0|              120|\n",
      "|je regarde la tél...|    0|               48|\n",
      "|je suis fatigué d...|    0|              133|\n",
      "|les deux adultes ...|    0|              111|\n",
      "|les étoiles dans ...|    0|              138|\n",
      "|nous avons vraime...|    0|              142|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "train_data_bis = remove_train.withColumn(\"length_of_message\", F.length(\"message\"))\n",
    "train_data_bis.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58abb8b7",
   "metadata": {},
   "source": [
    "# Mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c702810",
   "metadata": {},
   "source": [
    "Il serait intéressant de regarder les mots les plus fréquents et d'observer s'ils ont réellement une importance sur la classification des sentiments. S'il s'agit des pronoms indéfinis, définis, compléments... ils peuvent avoir très peu d'impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf72b6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:10.073169Z",
     "start_time": "2022-03-18T18:48:09.774732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "| comment|\n",
      "|   était|\n",
      "|      la|\n",
      "|   terre|\n",
      "|     des|\n",
      "|écossais|\n",
      "| daccord|\n",
      "|   vatil|\n",
      "|apporter|\n",
      "|     des|\n",
      "|   armes|\n",
      "|       à|\n",
      "|     feu|\n",
      "|      et|\n",
      "|     des|\n",
      "+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "train_word =  (train_data_bis\n",
    "                    .select(explode(split(train_data_bis.message,'[\\s]+')) \n",
    "                    .alias('word'))\n",
    "                    .where(\"word!=''\"))  \n",
    "train_word.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63d156",
   "metadata": {},
   "source": [
    "Chaque ligne représente un mot. Il est utile de définir cette variable $\\textbf{train_word}$ pour qu'on puisse regarder la fréquence des mots par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e974cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:13.701751Z",
     "start_time": "2022-03-18T18:48:13.682799Z"
    }
   },
   "outputs": [],
   "source": [
    "  def wordCount(wordListDF):\n",
    "        \"\"\"Crée un DataFrame avec le nombre de mots..\n",
    "\n",
    "        Args:\n",
    "           wordListDF (str): Un DataFrame composé d'une colonne de chaîne appelée 'word'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame of (str, int) : un DataFrame contenant les colonnes 'word' et 'count'.\n",
    "        \"\"\"\n",
    "        return wordListDF.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8da166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:17.843502Z",
     "start_time": "2022-03-18T18:48:15.304189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|        de|63026|\n",
      "|        je|60897|\n",
      "|        le|34041|\n",
      "|        la|32126|\n",
      "|         à|31028|\n",
      "|       pas|28539|\n",
      "|       que|27591|\n",
      "|        et|26238|\n",
      "|        un|21668|\n",
      "|      vous|20095|\n",
      "|        ne|19732|\n",
      "|      pour|19489|\n",
      "|       les|18223|\n",
      "|       est|14878|\n",
      "|        en|14072|\n",
      "|       jai|13849|\n",
      "|       une|13594|\n",
      "|      suis|13133|\n",
      "|       mon|13051|\n",
      "|        ce|12264|\n",
      "|        me|11398|\n",
      "|       des|11008|\n",
      "|      mais|10627|\n",
      "|         a|10558|\n",
      "|       sur| 9967|\n",
      "|      cest| 9588|\n",
      "|      avec| 9165|\n",
      "|      dans| 9084|\n",
      "|        il| 9059|\n",
      "|        au| 8920|\n",
      "|      plus| 7662|\n",
      "|        ma| 7585|\n",
      "|        du| 7394|\n",
      "|        si| 6852|\n",
      "|      bien| 6545|\n",
      "|     faire| 6450|\n",
      "|maintenant| 6447|\n",
      "|      nous| 5995|\n",
      "|aujourdhui| 5516|\n",
      "|      tout| 5451|\n",
      "|        se| 5217|\n",
      "|      fait| 5078|\n",
      "|        ça| 5038|\n",
      "|     merci| 4889|\n",
      "|       mes| 4784|\n",
      "|       qui| 4758|\n",
      "|      être| 4652|\n",
      "|       moi| 4588|\n",
      "| tellement| 4496|\n",
      "|       lol| 4456|\n",
      "+----------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topwords_train = wordCount(train_word).orderBy(['count'],ascending=False) \n",
    "topwords_train.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5a65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:30:01.696216Z",
     "start_time": "2022-03-18T18:30:01.680642Z"
    }
   },
   "source": [
    "On observe que parmis les tweets les mots les plus fréquents sont : de, je, le, la, à ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b017a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:25.076914Z",
     "start_time": "2022-03-18T18:48:19.459875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68107\n"
     ]
    }
   ],
   "source": [
    "uniqueWordsCount = topwords_train.distinct().groupBy().count().head()[0]\n",
    "print(uniqueWordsCount) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa68d27",
   "metadata": {},
   "source": [
    "On a plus de 68107 mots distincts dans notre base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b73af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:48:25.125784Z",
     "start_time": "2022-03-18T18:48:20.365Z"
    }
   },
   "outputs": [],
   "source": [
    "averageCount = topwords_train.groupBy().mean('count').head()[0]\n",
    "print(averageCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aae03",
   "metadata": {},
   "source": [
    "Il s'agit du nombre moyen d'occurrences de mots dans topwords_train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6fd44",
   "metadata": {},
   "source": [
    "# Traitement des tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a96945",
   "metadata": {},
   "source": [
    "$\\underline{Tokenisation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7b738",
   "metadata": {},
   "source": [
    "On applique la tokenisation qui consiste à prendre du texte (comme une phrase) et à le décomposer en termes individuels\n",
    "(généralement des mots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3425f07c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:52:03.507879Z",
     "start_time": "2022-03-18T18:52:02.694860Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+--------------------+\n",
      "|             message|label|length_of_message|               words|\n",
      "+--------------------+-----+-----------------+--------------------+\n",
      "|comment était la ...|    0|               35|[comment, était, ...|\n",
      "|daccord vatil app...|    0|              108|[daccord, vatil, ...|\n",
      "|taihen desu ne un...|    0|               51|[taihen, desu, ne...|\n",
      "|autodj  ce serait...|    0|               72|[autodj, , ce, se...|\n",
      "|ce nest que hier ...|    0|              119|[ce, nest, que, h...|\n",
      "+--------------------+-----+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, StopWordsRemover, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = \"message\", outputCol= \"words\")\n",
    "tokenized = tokenizer.transform(train_data_bis)\n",
    "tokenized.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06320e0",
   "metadata": {},
   "source": [
    "La nouvelle colonne \"words\" contient les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6f4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:38:09.299857Z",
     "start_time": "2022-03-18T18:38:09.283908Z"
    }
   },
   "source": [
    "$\\underline{StopWordsRemover}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155ca3e",
   "metadata": {},
   "source": [
    "On enlève ensuite les mots vides avec StopWordsRemover. Ce sont des mots qui doivent être exclus de l’entrée, gnéralement parce que les mots apparaissent fréquemment et n’ont pas autant de sens. La majorité des tweets sont en français, on va supprimer les mots vides en précisant la langue french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e788c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:52:13.864218Z",
     "start_time": "2022-03-18T18:52:13.858611Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwordList = nltk.corpus.stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74556b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:52:14.936491Z",
     "start_time": "2022-03-18T18:52:14.036394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+--------------------+--------------------+\n",
      "|             message|label|length_of_message|               words|          clean_word|\n",
      "+--------------------+-----+-----------------+--------------------+--------------------+\n",
      "|comment était la ...|    0|               35|[comment, était, ...|[comment, terre, ...|\n",
      "|daccord vatil app...|    0|              108|[daccord, vatil, ...|[daccord, vatil, ...|\n",
      "|taihen desu ne un...|    0|               51|[taihen, desu, ne...|[taihen, desu, id...|\n",
      "|autodj  ce serait...|    0|               72|[autodj, , ce, se...|[autodj, , peu, d...|\n",
      "|ce nest que hier ...|    0|              119|[ce, nest, que, h...|[nest, hier, jai,...|\n",
      "+--------------------+-----+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words_remove = StopWordsRemover(inputCol= tokenizer.getOutputCol(),\n",
    "                                    outputCol = \"clean_word\", stopWords=stopwordList)\n",
    "stop_words_remove_bis = stop_words_remove.transform(tokenized)\n",
    "stop_words_remove_bis.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a0e0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:53:05.270639Z",
     "start_time": "2022-03-18T18:53:05.043909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(message='comment était la terre des écossais', label=0, length_of_message=35, words=['comment', 'était', 'la', 'terre', 'des', 'écossais'], clean_word=['comment', 'terre', 'écossais'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_remove_bis.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61b0c0",
   "metadata": {},
   "source": [
    "Comme on peut le voir, lorsqu'on affiche le premier tweet, les mots \"était\", \"la\", \"des\" ont été supprimés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9639c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:39:05.316794Z",
     "start_time": "2022-03-18T18:39:05.297765Z"
    }
   },
   "source": [
    "$\\underline{HashingTF}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65d649",
   "metadata": {},
   "source": [
    "On convertit ensuite les vecteurs de mots sous forme de valeur numérique pour la modélisation. On utilise HashingTF.\n",
    "\n",
    "HashingTF est un transformateur qui prend des ensembles de termes et convertit ces ensembles en vecteurs de caractéristiques de longueur fixe. Dans le traitement de texte, un \"ensemble de termes\" peut être un sac de mots. HashingTF utilise l’astuce de hachage. Une caractéristique brute est mappée dans un index (terme) en appliquant une fonction de hachage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1624da9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:53:19.070013Z",
     "start_time": "2022-03-18T18:53:18.439237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "|             message|length_of_message|          clean_word|       transform_num|label|\n",
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "|comment était la ...|               35|[comment, terre, ...|(262144,[3086,178...|    0|\n",
      "|daccord vatil app...|              108|[daccord, vatil, ...|(262144,[60211,10...|    0|\n",
      "|taihen desu ne un...|               51|[taihen, desu, id...|(262144,[151360,1...|    0|\n",
      "|autodj  ce serait...|               72|[autodj, , peu, d...|(262144,[31950,47...|    0|\n",
      "|ce nest que hier ...|              119|[nest, hier, jai,...|(262144,[2967,440...|    0|\n",
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=stop_words_remove.getOutputCol(), outputCol=\"transform_num\")\n",
    "numericDataTrain = hashTF.transform(stop_words_remove_bis).select(\n",
    "\"message\",\"length_of_message\" ,\"clean_word\", \"transform_num\", \"label\" )\n",
    "numericDataTrain.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d51844",
   "metadata": {},
   "source": [
    "# Données Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1070e",
   "metadata": {},
   "source": [
    "On fait le même procédé avec les données test : $\\textbf{test.json}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b40b3d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:54:27.222414Z",
     "start_time": "2022-03-18T18:54:25.958195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "|             message|length_of_message|          clean_word|       transform_num|label|\n",
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "|et si affamé mais...|               67|[si, affamé, souc...|(262144,[16672,74...|    0|\n",
      "|identica présente...|              140|[identica, présen...|(262144,[7054,328...|    0|\n",
      "|je vais enfin men...|               80|[vais, enfin, men...|(262144,[16620,35...|    0|\n",
      "+--------------------+-----------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = spark.read.json(\"C:/Users/na_to/OneDrive/Bureau/Insa/Mapromo/Calcul_distribue/Projet/test.json\")\n",
    "#4 partitions sur l'importation de données : voir http://localhost:4040 donc données déjà distribué RDD \n",
    "\n",
    "test_data = test_data.select(\"message\", col(\"polarity\").cast(\"Int\").alias(\"label\"))\n",
    "\n",
    "test_data = test_data.select(removePunctuation(col('message')),\"label\")\n",
    "\n",
    "#ajout variable length_of_message \n",
    "test_data_bis = test_data.withColumn(\"length_of_message\", F.length(\"message\"))\n",
    "\n",
    "#Liste de mots\n",
    "tokenizedTest = tokenizer.transform(test_data_bis)\n",
    "\n",
    "#enleve mots vide\n",
    "stop_words_remove_test = stop_words_remove.transform(tokenizedTest)\n",
    "\n",
    "numericTestData = hashTF.transform(stop_words_remove_test).select(\n",
    "\"message\",\"length_of_message\" ,\"clean_word\", \"transform_num\", \"label\" )\n",
    "numericTestData.show(truncate=True, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b621d",
   "metadata": {},
   "source": [
    "# Préparation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3038201d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:54:30.312580Z",
     "start_time": "2022-03-18T18:54:30.295314Z"
    }
   },
   "outputs": [],
   "source": [
    "#On construit ensuite un vecteur rassemblant toutes les variables explicatives\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#On rassemble la liste des colonnes numériques que l'on va utiliser\n",
    "numericCols = ['length_of_message','transform_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf6eb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:54:30.954275Z",
     "start_time": "2022-03-18T18:54:30.935309Z"
    }
   },
   "outputs": [],
   "source": [
    "#On crée un objet qui rassemble toutes ces colonnes dans une colonne nommée features\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff1f19",
   "metadata": {},
   "source": [
    "# Modèle regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0852b3",
   "metadata": {},
   "source": [
    "J'utilise la bibliothèque spark.ml qui permet de faire du machine learning sur les DataFrame. La bibliotéque spark.ml\n",
    "ressemble beaucoup par ses principes à scikit-learn. On crée des objets en utilisant des classes spécifiques et on applique des méthodes .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9e4570a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T18:54:36.905424Z",
     "start_time": "2022-03-18T18:54:36.849011Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#On crée notre modèle \n",
    "model_reg = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "from pyspark.ml import Pipeline \n",
    "#On construit le pipeline qui est composé des 2 étapes développées auparavant \n",
    "pipeline = Pipeline(stages=[assembler, model_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "678db328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:03:20.419141Z",
     "start_time": "2022-03-18T18:54:37.925221Z"
    }
   },
   "outputs": [],
   "source": [
    "#ajustement du model\n",
    "model_reg = pipeline.fit(numericDataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59d1e767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:03:41.863427Z",
     "start_time": "2022-03-18T19:03:41.013636Z"
    }
   },
   "outputs": [],
   "source": [
    "#prévision sur les données de validation\n",
    "predictions_reg = model_reg.transform(numericTestData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e37f3",
   "metadata": {},
   "source": [
    "Par défaut spark va créer de nouvelles colonnes dans nos données avec les prédictions (colonne prediction) et les probabilitées de prédiction (colonne rawPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae247c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:04:34.347540Z",
     "start_time": "2022-03-18T19:04:32.060404Z"
    }
   },
   "outputs": [],
   "source": [
    "#Sauvegarde du modèle \n",
    "model_reg.save(\"C:/Users/na_to/OneDrive/Bureau/Insa/Mapromo/Calcul_distribue/Projet/logistic_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4025468",
   "metadata": {},
   "source": [
    "Le modèle est sauvegardé pour éviter de refaire le fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcd02a",
   "metadata": {},
   "source": [
    "$\\underline{Evaluation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72f4d7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:05:00.734012Z",
     "start_time": "2022-03-18T19:04:37.922270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5423502204199602"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#Cette classe calcule l'AUC de notre modèle\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "#On applique les données prédites à notre objet d'évaluation\n",
    "evaluator.evaluate(predictions_reg)\n",
    "#L'AUC est affiché "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "666d2e96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:05:03.225337Z",
     "start_time": "2022-03-18T19:05:03.029409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|          clean_word|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[si, affamé, souc...|    0|       0.0|\n",
      "|[identica, présen...|    0|       4.0|\n",
      "|[vais, enfin, men...|    0|       0.0|\n",
      "|[cest, jour, jai,...|    0|       0.0|\n",
      "|[easy, plancher, ...|    0|       0.0|\n",
      "|[empire, soleil, ...|    0|       0.0|\n",
      "|[heart, quot, nes...|    0|       0.0|\n",
      "|[i, need, , quot,...|    0|       4.0|\n",
      "|[trouvé, sonny, ,...|    0|       4.0|\n",
      "|[vérifie, twitter...|    0|       4.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsfinal = predictions_reg.select(\n",
    "\"clean_word\", \"label\", \"prediction\")\n",
    "predictionsfinal.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf70950f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:05:25.117724Z",
     "start_time": "2022-03-18T19:05:05.160145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 54259 , total data: 76759 , accuracy: 0.7068747638713375\n"
     ]
    }
   ],
   "source": [
    "correctpredictions = predictionsfinal.filter(predictionsfinal[\"prediction\"] == predictionsfinal[\"label\"]).count()\n",
    "totalData = predictionsfinal.count()\n",
    "print(\"correct prediction:\", correctpredictions, \", total data:\", totalData, \", accuracy:\" ,correctpredictions/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1abe9e",
   "metadata": {},
   "source": [
    "On obtient un accuracy d'environ 70%, presque 3 quart des tweet ont été bien classé sur les données test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3a6fa",
   "metadata": {},
   "source": [
    "# Data no class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0aebed66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:05:35.217893Z",
     "start_time": "2022-03-18T19:05:34.987395Z"
    }
   },
   "outputs": [],
   "source": [
    "noclass_data = spark.read.json(\"C:/Users/na_to/OneDrive/Bureau/Insa/Mapromo/Calcul_distribue/Projet/noclass.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89981638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:05:37.092916Z",
     "start_time": "2022-03-18T19:05:37.019946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             message|\n",
      "+--------------------+\n",
      "|\"Dans ganga\" Ne v...|\n",
      "|\"Dieu elton\" vous...|\n",
      "|\"I was up up the ...|\n",
      "|\"Quasi\" toute la ...|\n",
      "|# $ & Amp; * # vi...|\n",
      "|& Amp; Je parle d...|\n",
      "|& Amp; nd je ne p...|\n",
      "|& Gt; Pas bon à p...|\n",
      "|& Lt; 3 il semble...|\n",
      "|& Lt; 3 shopping ...|\n",
      "|& Quot si la lice...|\n",
      "|& Quot; can not s...|\n",
      "|& Quot; je vais g...|\n",
      "|& Quot; joellen a...|\n",
      "|& Quot; page not ...|\n",
      "|& Quot; si vous ê...|\n",
      "|&quot;une facture...|\n",
      "|'S foot is endorm...|\n",
      "|() 'Jizzed dans m...|\n",
      "|() J'ai l'impress...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noclass_data.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268e1fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:06:06.618188Z",
     "start_time": "2022-03-18T19:06:06.467960Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove punctuation  \n",
    "noclass_data_bis = noclass_data.select(removePunctuation(col('message')))\n",
    "\n",
    "#ajout variable length_of_message \n",
    "noclass_data_bis = noclass_data_bis.withColumn(\"length_of_message\", F.length(\"message\"))\n",
    "\n",
    "#Liste de mots\n",
    "tokenizedNoclass = tokenizer.transform(noclass_data_bis)\n",
    "\n",
    "#enleve mots vide\n",
    "stop_words_remove_noclass = stop_words_remove.transform(tokenizedNoclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ded092df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:06:09.549900Z",
     "start_time": "2022-03-18T19:06:09.410273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+\n",
      "|             message|length_of_message|          clean_word|       transform_num|\n",
      "+--------------------+-----------------+--------------------+--------------------+\n",
      "|dans ganga ne veu...|              101|[ganga, veuxtu, d...|(262144,[13779,42...|\n",
      "|dieu elton vous n...|              159|[dieu, elton, pou...|(262144,[10504,76...|\n",
      "|i was up up the h...|              118|[i, was, up, up, ...|(262144,[17252,24...|\n",
      "|quasi toute la nu...|               76|[quasi, toute, nu...|(262144,[65739,76...|\n",
      "|amp   vient dache...|              156|[amp, , , vient, ...|(262144,[21832,37...|\n",
      "+--------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numericNoclassData = hashTF.transform(stop_words_remove_noclass).select(\n",
    "\"message\",\"length_of_message\" ,\"clean_word\", \"transform_num\" )\n",
    "numericNoclassData.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10ad2bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:06:37.334636Z",
     "start_time": "2022-03-18T19:06:36.427888Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_noclass = model_reg.transform(numericNoclassData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84a1ed7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:06:39.779979Z",
     "start_time": "2022-03-18T19:06:39.495131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|             message|length_of_message|          clean_word|       transform_num|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|dans ganga ne veu...|              101|[ganga, veuxtu, d...|(262144,[13779,42...|(262145,[0,13780,...|[11.4306507822779...|[0.99987920178591...|       0.0|\n",
      "|dieu elton vous n...|              159|[dieu, elton, pou...|(262144,[10504,76...|(262145,[0,10505,...|[26.5138823979166...|[0.99999999999991...|       0.0|\n",
      "|i was up up the h...|              118|[i, was, up, up, ...|(262144,[17252,24...|(262145,[0,17253,...|[3.33330455891403...|[6.54357162137833...|       4.0|\n",
      "|quasi toute la nu...|               76|[quasi, toute, nu...|(262144,[65739,76...|(262145,[0,65740,...|[6.19077713342584...|[0.20525328900436...|       4.0|\n",
      "|amp   vient dache...|              156|[amp, , , vient, ...|(262144,[21832,37...|(262145,[0,21833,...|[11.0072074397229...|[0.99957909279797...|       0.0|\n",
      "|amp je parle de m...|               32|[amp, parle, derr...|(262144,[73440,12...|(262145,[0,73441,...|[6.44894351253861...|[0.36065092816194...|       4.0|\n",
      "|amp nd je ne peux...|               63|[amp, nd, peux, t...|(262144,[75925,80...|(262145,[0,75926,...|[6.68368701777512...|[0.43317734885563...|       4.0|\n",
      "|gt pas bon à prop...|              132|[gt, bon, propos,...|(262144,[22136,22...|(262145,[0,22137,...|[-2.8351738579039...|[3.11807677782053...|       4.0|\n",
      "|lt 3 il semble qu...|               69|[lt, 3, semble, p...|(262144,[53719,89...|(262145,[0,53720,...|[6.91136256981622...|[0.53871713544968...|       0.0|\n",
      "|lt 3 shopping plu...|               97|[lt, 3, shopping,...|(262144,[21905,36...|(262145,[0,21906,...|[3.58275725143646...|[0.00110427070143...|       4.0|\n",
      "|quot si la licenc...|              138|[quot, si, licenc...|(262144,[63191,76...|(262145,[0,63192,...|[7.80096212166322...|[0.84156427973132...|       0.0|\n",
      "|quot can not stac...|              159|[quot, can, not, ...|(262144,[26386,36...|(262145,[0,26387,...|[7.07257107600719...|[0.48758757764888...|       4.0|\n",
      "|quot je vais gifl...|              135|[quot, vais, gifl...|(262144,[19510,62...|(262145,[0,19511,...|[4.55524768340709...|[0.00772644904831...|       4.0|\n",
      "|quot joellen a co...|               96|[quot, joellen, a...|(262144,[5153,992...|(262145,[0,5154,9...|[5.43798496009368...|[0.05291794929691...|       4.0|\n",
      "|quot page not fou...|               25|[quot, page, not,...|(262144,[76332,11...|(262145,[0,76333,...|[4.72707827665868...|[0.01593642629787...|       4.0|\n",
      "|quot si vous êtes...|              145|[quot, si, us, ci...|(262144,[21872,65...|(262145,[0,21873,...|[18.3619189798252...|[0.99999999956841...|       0.0|\n",
      "|quotune facture d...|               93|[quotune, facture...|(262144,[3360,963...|(262145,[0,3361,9...|[4.35315264528386...|[0.00648919948475...|       4.0|\n",
      "|s foot is endorm  ow|               20|[foot, is, endorm...|(262144,[15889,13...|(262145,[0,15890,...|[8.53426281470824...|[0.97480560916679...|       0.0|\n",
      "|jizzed dans mon p...|              150|[jizzed, pantalon...|(262144,[7025,152...|(262145,[0,7026,1...|[-3.1854454154502...|[1.37670017397143...|       4.0|\n",
      "|jai limpression q...|               50|[jai, limpression...|(262144,[44075,86...|(262145,[0,44076,...|[9.03125577484796...|[0.98815312653008...|       0.0|\n",
      "+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_noclass.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d025b5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T19:08:24.197799Z",
     "start_time": "2022-03-18T19:07:08.740960Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_noclass.coalesce(1).write.format('json').save(\"C:/Users/na_to/OneDrive/Bureau/Insa/Mapromo/Calcul_distribue/Projet/noclass_bis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f98e06",
   "metadata": {},
   "source": [
    "On sauvegarde en format json la base de données predictions_noclass en le nommant noclass_bis.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde96b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
